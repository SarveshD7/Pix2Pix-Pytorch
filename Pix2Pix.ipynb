{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNxMt534Bs9jewCOaDSfuzy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SarveshD7/Pix2Pix-Pytorch/blob/main/Pix2Pix.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Important Points on the Architecture of Pix2Pix***\n",
        "\n",
        "Generator is very much inspired by U-Net\n",
        "\n",
        "Discriminator is just a couple of Convolutional Layers\n",
        "\n",
        "Discriminator- PatchGAN - The output of the discriminator is not a single value representing whether the image is real or fake.\n",
        "\n",
        "Rather it is a grid like image (here 30x30 or 70x70 or 26x26) where each value is between [0,1] and represents whether a particular patch of the original image is real or fake.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ykdNV5EgeF5n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "b9Gylhq0LPED"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LEARNING_RATE = 2e-4\n",
        "BATCH_SIZE = 16\n",
        "NUM_WORKERS = 2\n",
        "IMAGE_SIZE = 256\n",
        "CHANNELS_IMG = 3\n",
        "L1_LAMBDA = 100\n",
        "NUM_EPOCHS = 500\n",
        "LOAD_MODEL = False\n",
        "SAVE_MODEL = True\n"
      ],
      "metadata": {
        "id": "Rr7UjZaA7kLi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "both_transform = A.Compose(\n",
        "    [A.Resize(width=256, height=256),], additional_targets={\"image0\": \"image\"},\n",
        ")\n",
        "\n",
        "transform_only_input = A.Compose(\n",
        "    [\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.ColorJitter(p=0.2),\n",
        "        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], max_pixel_value=255.0,),\n",
        "        ToTensorV2(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "transform_only_mask = A.Compose(\n",
        "    [\n",
        "        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], max_pixel_value=255.0,),\n",
        "        ToTensorV2(),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "7ARUtqEv8Cv3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MapDataset(Dataset):\n",
        "  def __init__(self, root_dir):\n",
        "    self.root_dir = root_dir\n",
        "    self.list_files = os.listdir(self.root_dir)\n",
        "    print(self.list_files)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.list_files)\n",
        "\n",
        "  def __getItem__(self, index):\n",
        "    img_file = self.list_files[index]\n",
        "    img_path = os.path.join(self.root_dir, img_file)\n",
        "    img = np.array(Image.open(img_path))\n",
        "    input_img = img[:, :600, :]  # Since the image comprises of input image and the target joined along the width so taking only till 600 we take the input image only\n",
        "    target_img = img[:, 600: , :]  # Taking the target image only\n",
        "\n",
        "    augmentations = both_transform(image=input_img, image0=target_img)\n",
        "    input_img, target_img = augmentations[\"image\"], augmentations[\"image0\"]\n",
        "\n",
        "    input_img = transform_only_input(image=input_img)[\"image\"]\n",
        "    target_img = transform_only_mask(image=input_img)[\"image\"]\n",
        "\n",
        "    return input_img, target_img"
      ],
      "metadata": {
        "id": "edA9gDu6fplR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNBlock(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, stride=2):\n",
        "    super().__init__()\n",
        "    self.conv = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, 4, stride, bias=False, padding_mode=\"reflect\"),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.LeakyReLU(0.2)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.conv(x)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "  def __init__(self, in_channels, features=[64, 128, 256, 512]):\n",
        "    #  features is used to call the same conv once on each of the values\n",
        "    super().__init__()\n",
        "    self.initial = nn.Sequential(\n",
        "        # The input size of Conv2d is in_channels*2 because We pass input image and target output concatenated on the channels\n",
        "        nn.Conv2d(in_channels*2, features[0], kernel_size=4, stride=2, padding=1, padding_mode=\"reflect\"),\n",
        "        nn.LeakyReLU(0.2)\n",
        "    )\n",
        "\n",
        "    layers = []\n",
        "    in_channels = features[0]\n",
        "    for feature in features[1:]:\n",
        "      layers.append(\n",
        "          CNNBlock(in_channels, feature, stride=1 if feature==features[-1] else 2)\n",
        "      )\n",
        "      in_channels = feature\n",
        "    layers.append(\n",
        "        nn.Conv2d(in_channels, 1, kernel_size=4, stride=1, padding=1, padding_mode=\"reflect\")\n",
        "    )\n",
        "    self.model = nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self,x, y):\n",
        "    x = torch.cat([x, y], dim=1)\n",
        "    x = self.initial(x)\n",
        "    return self.model(x)\n",
        "\n",
        "def test():\n",
        "  x = torch.randn((1, 3, 256, 256))\n",
        "  y = torch.randn((1, 3, 256, 256))\n",
        "  model = Discriminator(3)\n",
        "  preds = model(x, y)\n",
        "  print(preds.shape)\n",
        "test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6pwp9U7iuw5",
        "outputId": "2071fab3-da30-4bbb-ccb0-80d8a5cbe805"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 26, 26])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, down=True, act=\"relu\", use_dropout=False):\n",
        "    super().__init__()\n",
        "    self.conv = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, 4,2,1,bias=False, padding_mode=\"reflect\")\n",
        "        if down\n",
        "        else nn.ConvTranspose2d(in_channels, out_channels, 4,2,1,bias=False),\n",
        "        nn.ReLU() if act==\"relu\" else nn.LeakyReLU(0.2),\n",
        "    )\n",
        "    self.use_dropout = use_dropout\n",
        "    self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv(x)\n",
        "    return self.dropout(x) if self.use_dropout else x\n",
        "\n",
        "class Generator(nn.Module):\n",
        "  def __init__(self, in_channels, features=64):\n",
        "    super().__init__()\n",
        "    self.initial_down = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, features, 4, 2, 1, padding_mode=\"reflect\"),\n",
        "        nn.LeakyReLU(0.2)\n",
        "    )  # 128 x 128\n",
        "\n",
        "    self.down1 = Block(features, features*2, down=True, act=\"leaky\", use_dropout=False)  # 64 x 64\n",
        "    self.down2 = Block(features*2, features*4, down=True, act=\"leaky\", use_dropout=False) # 32 x 32\n",
        "    self.down3 = Block(features*4, features*8, down=True, act=\"leaky\", use_dropout=False)  # 16 x 16\n",
        "    self.down4 = Block(features*8, features*8, down=True, act=\"leaky\", use_dropout=False)  # 8 x 8\n",
        "    self.down5 = Block(features*8, features*8, down=True, act=\"leaky\", use_dropout=False)  # 4 x 4\n",
        "    self.down6 = Block(features*8, features*8, down=True, act=\"leaky\", use_dropout=False)  # 2 x 2\n",
        "\n",
        "    self.bottleneck = nn.Sequential(\n",
        "        nn.Conv2d(features*8, features*8, 4,2,1, padding_mode=\"reflect\"),\n",
        "        nn.ReLU()  # 1 x 1\n",
        "    )\n",
        "    self.up1 = Block(features*8, features*8, down=False, act=\"relu\", use_dropout=True)\n",
        "    self.up2 = Block(features*8*2, features*8, down=False, act=\"relu\", use_dropout=True)\n",
        "    self.up3 = Block(features*8*2, features*8, down=False, act=\"relu\", use_dropout=True)\n",
        "    self.up4 = Block(features*8*2, features*8, down=False, act=\"relu\", use_dropout=False)\n",
        "    self.up5 = Block(features*8*2, features*4, down=False, act=\"relu\", use_dropout=False)\n",
        "    self.up6 = Block(features*4*2, features*2, down=False, act=\"relu\", use_dropout=False)\n",
        "    self.up7 = Block(features*2*2, features, down=False, act=\"relu\", use_dropout=False)\n",
        "    self.final_up = nn.Sequential(\n",
        "        nn.ConvTranspose2d(features*2, in_channels, kernel_size=4, stride=2, padding=1),\n",
        "        nn.Tanh()\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    d1 = self.initial_down(x)\n",
        "    d2 = self.down1(d1)\n",
        "    d3 = self.down2(d2)\n",
        "    d4 = self.down3(d3)\n",
        "    d5 = self.down4(d4)\n",
        "    d6 = self.down5(d5)\n",
        "    d7 = self.down6(d6)\n",
        "    bottleneck = self.bottleneck(d7)\n",
        "    up1 = self.up1(bottleneck)\n",
        "    up2 = self.up2(torch.cat([up1, d7], 1))\n",
        "    up3 = self.up3(torch.cat([up2, d6], 1))\n",
        "    up4 = self.up4(torch.cat([up3, d5], 1))\n",
        "    up5 = self.up5(torch.cat([up4, d4], 1))\n",
        "    up6 = self.up6(torch.cat([up5, d3], 1))\n",
        "    up7 = self.up7(torch.cat([up6, d2], 1))\n",
        "    return self.final_up(torch.cat([up7, d1], 1))\n"
      ],
      "metadata": {
        "id": "-S9uEXyJ5m_X"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test():\n",
        "  x = torch.randn((1, 3, 256, 256))\n",
        "  model = Generator(in_channels=3, features=64)\n",
        "  preds = model(x)\n",
        "  print(preds.shape)"
      ],
      "metadata": {
        "id": "OV0Kn0OiCbhw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1CFR-5LEYmp",
        "outputId": "724e259d-3684-4ca2-e846-3d1fbc35221b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3, 256, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(disc, gen, loader, opt_disc, opt_gen, l1, bce):\n",
        "  for idx, (x, y) in loader:\n",
        "\n",
        "    # Train Discriminator\n",
        "    y_fake = gen(x)\n",
        "    D_real = disc(x, y)\n",
        "    D_fake = disc(x,y_fake.detach())\n",
        "    D_real_loss = bce(D_real, torch.ones_like(D_real))\n",
        "    D_fake_loss = bce(D_fake, torch.ones_like(D_fake))\n",
        "    D_loss = (D_real_loss+D_fake_loss)/2\n",
        "\n",
        "    disc.zero_grad()\n",
        "    D_loss.backward()\n",
        "    disc.step(opt_disc)\n",
        "\n",
        "    # Train Generator\n",
        "    D_fake = disc(x, y_fake)\n",
        "    G_fake_loss = bce(D_fake, torch.ones_like(D_fake))\n",
        "    L1 = l1(y_fake, y)*L1_LAMBDA\n",
        "    G_loss = G_fake_loss+l1\n",
        "    gen.zero_grad()\n",
        "    G_loss.backward()\n",
        "    gen.step(opt_gen)\n",
        "\n"
      ],
      "metadata": {
        "id": "mNxHy-BlCP2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disc = Discriminator(in_channels=3)\n",
        "gen = Generator(in_channels=3)\n",
        "opt_disc = optim.Adam(disc.parameters(), lr = LEARNING_RATE, betas = (0.5, 0.999))\n",
        "opt_gen = optim.Adam(gen.parameters(), lr = LEARNING_RATE, betas = (0.5, 0.999))\n",
        "BCE = nn.BCEWithLogitsLoss()\n",
        "L1_LOSS = nn.L1Loss()\n",
        "\n",
        "train_dataset = MapDataset(root_dir=\"/content/data/maps/train\")\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
        "\n",
        "val_dataset = MapDataset(root_dir=\"/content/data/maps/val\")\n",
        "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  train(disc, gen, train_loader, opt_disc, opt_gen, L1_LOSS, BCE)"
      ],
      "metadata": {
        "id": "rbY3HjJIFCEs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}